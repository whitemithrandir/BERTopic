{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXHLDxJdRzBi"
   },
   "source": [
    "# **Tutorial** - (Custom) Embedding Models in BERTopic\n",
    "(last updated 11-09-2022)\n",
    "\n",
    "In this tutorial we will be going through the embedding models that can be used in BERTopic. Having the option to choose embedding models allow you to leverage pre-trained embeddings that suit your use-case. Moreover, it helps creating a topic when you have little data to your availability. \n",
    "\n",
    "## Embedding models\n",
    "Embedding models are used for the representation of text, such as words, sentences and documents, in the form of real-valued vectors. They typically encode the semantic meaning of text. \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MaartenGr/BERTopic/master/images/logo.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9Xfz5COO-5a"
   },
   "source": [
    "# Enabling the GPU\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "[Reference](https://colab.research.google.com/notebooks/gpu.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeFueQ8xP_7E"
   },
   "source": [
    "# Installing BERTopic\n",
    "\n",
    "We start by installing BERTopic, with all backends possible, from PyPi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SNa-KtKDRnus"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bertopic[flair, gensim, spacy, use]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wb6HEJHUj9Wn"
   },
   "source": [
    "**NOTE**: This may take a while as it needs to install Spacy, Torch, Gensim, USE, etc. \n",
    "\n",
    "**NOTE 1**: There might be dependency-conflicts if you install back-ends so it might be worthwhile to only choose one to experiment with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeg8RIjrQFkE"
   },
   "source": [
    "## Restart the Notebook\n",
    "After installing BERTopic, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.\n",
    "\n",
    "From the Menu:\n",
    "\n",
    "Runtime â†’ Restart Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3VGFZ1USMTu"
   },
   "source": [
    "# **Data**\n",
    "For this example, we use the popular 20 Newsgroups dataset which contains roughly 18000 newsgroups posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JJij3WP6SEQD"
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "docs = fetch_20newsgroups(subset='train',  remove=('headers', 'footers', 'quotes'))['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3v3bdLERcxG",
    "outputId": "dc65573b-e724-405e-8579-cc6217b4de4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBcNmZJzSTY8"
   },
   "source": [
    "# **Embedding Models**\n",
    "In this section, we will go through all embedding models and backends that are supported in BERTopic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNoAyX1CSHRh"
   },
   "source": [
    "## Sentence Transformers\n",
    "You can select any model from sentence-transformers [here](https://www.sbert.net/docs/pretrained_models.html) and pass it through BERTopic with `embedding_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfhfzqkoSJ1I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)252d8/.gitattributes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 795/795 [00:00<00:00, 199kB/s]\n",
      "Downloading (â€¦)_Pooling/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 38.0kB/s]\n",
      "Downloading (â€¦)ea1cc252d8/README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.01k/4.01k [00:00<00:00, 573kB/s]\n",
      "Downloading (â€¦)1cc252d8/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 722/722 [00:00<00:00, 103kB/s]\n",
      "Downloading (â€¦)ce_transformers.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122/122 [00:00<00:00, 17.4kB/s]\n",
      "Downloading (â€¦)\"pytorch_model.bin\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.11G/1.11G [05:45<00:00, 3.22MB/s]\n",
      "Downloading (â€¦)nce_bert_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53.0/53.0 [00:00<00:00, 7.57kB/s]\n",
      "Downloading (â€¦)ncepiece.bpe.model\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.07M/5.07M [00:01<00:00, 2.80MB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 25.0kB/s]\n",
      "Downloading (â€¦)\"tokenizer.json\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.10M/9.10M [00:02<00:00, 3.07MB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:00<00:00, 129kB/s]\n",
      "Downloading (â€¦)cc252d8/modules.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229/229 [00:00<00:00, 57.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(embedding_model=\"xlm-r-bert-base-nli-stsb-mean-tokens\").fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "QK58oUIJEYYv",
    "outputId": "1c83af67-e4bb-4569-f308-2282becdf6f2"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ua80usww-rj"
   },
   "source": [
    "Or we can select a SentenceTransformer model with our own parameters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUNsYrCETNar"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\", device=\"cuda\")\n",
    "topic_model = BERTopic(embedding_model=sentence_model).fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "iUDXJ882EfNa",
    "outputId": "9818f09c-bfff-459f-a5ad-9d85625de4dc"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DM8PG6KjFop"
   },
   "source": [
    "## ðŸ¤— Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmPa0acZjVVE"
   },
   "source": [
    "To use a Hugging Face transformers model, load in a pipeline and point to any model found on their model hub (https://huggingface.co/models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOHe5-IujWT1"
   },
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "\n",
    "embedding_model = pipeline(\"feature-extraction\", model=\"distilbert-base-cased\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model).fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zxlr8riQCdxz"
   },
   "source": [
    "## Flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYbCyFqKEjNK"
   },
   "source": [
    "Flair allows you to choose almost any embedding model that is publicly available.<br> Flair can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khqzs5D0Ejda"
   },
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "\n",
    "roberta = TransformerDocumentEmbeddings('roberta-base')\n",
    "topic_model = BERTopic(embedding_model=roberta).fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "qjY1BevyFtQO",
    "outputId": "8f401798-ce5e-4c94-9ab7-7e3e66a003a3"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPbDc22-EpGS"
   },
   "source": [
    "You can select any ðŸ¤— transformers model [here](https://huggingface.co/models).\n",
    "\n",
    "Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily pass it to BERTopic in order to use those word embeddings as document embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpl45ggKEtpH"
   },
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
    "\n",
    "glove_embedding = WordEmbeddings('crawl')\n",
    "document_glove_embeddings = DocumentPoolEmbeddings([glove_embedding])\n",
    "\n",
    "topic_model = BERTopic(embedding_model=document_glove_embeddings).fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "vcDTOBydHxEf",
    "outputId": "ac9fdd6a-0eb3-48f0-dc37-46cc17f0ebb7"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaJNMON2CkAA"
   },
   "source": [
    "## Spacy\n",
    "Spacy has shown great promise over the last years and is now slowly transitioning into transformer-based techniques which makes it interesting to use in BERTopic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJITc16IKyJL"
   },
   "source": [
    "We start by using a non-transformer-based model which we will have to download first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArkUy-yDLHTv"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7K3v2ZGLbhF"
   },
   "source": [
    "Next, simply load the model into a Spacy nlp instance and pass it through BERTopic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdBn0ZwDKoCX"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
    "\n",
    "topic_model = BERTopic(embedding_model=nlp, verbose=True).fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "oTSBcvHeML30",
    "outputId": "743140b7-e5ca-42a2-8b69-20512c15ba7c"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55uszcvXLSBX"
   },
   "source": [
    "We can also use their transformer-based models which we also have to download first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYFi_CSyLXje"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiweZ01jLY7P"
   },
   "source": [
    "As before, we simply load the model and pass it through BERTopic. Note that we exclude a bunch of features as they are not used in BERTopic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZCSiY_03zX0"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_trf\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
    "topic_model = BERTopic(embedding_model=nlp, verbose=True, min_topic_size=5).fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYXvdEBqLxv_"
   },
   "source": [
    "If you run into memory issues with spacy-transformer models, try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVcvwtKfLv4c"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from thinc.api import set_gpu_allocator, require_gpu\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
    "set_gpu_allocator(\"pytorch\")\n",
    "require_gpu(0)\n",
    "\n",
    "topic_model = BERTopic(embedding_model=nlp, verbose=True).fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2AaTdNhCkGO"
   },
   "source": [
    "## Universal Sentence Encoder (USE)\n",
    "The Universal Sentence Encoder encodes text into high dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGAmTJp7CDHe"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub\n",
    "embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "topic_model = BERTopic(verbose=True, embedding_model=embedding_model).fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "-WJF53JsF2zb",
    "outputId": "f60a0f03-d58d-40f8-cad5-62a59be5337f"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSegsqnUCkDH"
   },
   "source": [
    "## Gensim\n",
    "For Gensim, BERTopic supports its `gensim.downloader` module. Here, we can download any model word embedding model to be used in BERTopic. Note that Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evwmYnNdcPSQ"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "ft = api.load('fasttext-wiki-news-subwords-300')\n",
    "topic_model = BERTopic(verbose=True, embedding_model=ft).fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCBkN39qCrPt"
   },
   "source": [
    "# **Customization**\n",
    "Over the last years, many new embedding models have been released that could be interesting to use as a backend in BERTopic. It is not always feasible to implement them all as there are simply too many to follow. \n",
    "\n",
    "In order to still allow to use those embeddings, BERTopic knows several ways to add these embeddings while still allowing for full functionality of BERTopic. \n",
    "\n",
    "Moreover, there are several customization options that allow for a bit more control over which embedding to use when. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufgp3gi4CrSR"
   },
   "source": [
    "## Word + Document Embeddings\n",
    "You might want to be using different language models for creating document- and word-embeddings.\n",
    "For example, while SentenceTransformers might be great in embedding sentences and documents, you\n",
    "might prefer to use FastText to create the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VL8vW58bhMQl"
   },
   "outputs": [],
   "source": [
    "from bertopic.backend import WordDocEmbedder\n",
    "import gensim.downloader as api\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Word embedding model\n",
    "ft = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# Document embedding model\n",
    "distilbert = SentenceTransformer(\"distilbert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "# Create a model that uses both language models and pass it through BERTopic\n",
    "word_doc_embedder = WordDocEmbedder(embedding_model=distilbert, word_embedding_model=ft)\n",
    "topic_model = BERTopic(verbose=True, embedding_model=word_doc_embedder).fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "b-6-HuK8okas",
    "outputId": "c0dfddef-5e65-4aea-baf4-b2d3900ac0de"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jmYK2HvCrU-"
   },
   "source": [
    "## Custom Backend\n",
    "If your backend or model cannot be found in the ones currently available, you can use the BaseEmbedder\n",
    "class to create your own backend. Below, you will find an example of creating a SentenceTransformer backend for BERTopic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5zpxr31hS97"
   },
   "outputs": [],
   "source": [
    "from bertopic.backend import BaseEmbedder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CustomEmbedder(BaseEmbedder):\n",
    "    def __init__(self, embedding_model):\n",
    "        super().__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def embed(self, documents, verbose=False):\n",
    "        embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
    "        return embeddings \n",
    "\n",
    "# Create custom backend\n",
    "distilbert = SentenceTransformer(\"distilbert-base-nli-stsb-mean-tokens\")\n",
    "custom_embedder = CustomEmbedder(embedding_model=distilbert)\n",
    "\n",
    "# Pass custom backend to bertopic\n",
    "topic_model = BERTopic(embedding_model=custom_embedder).fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "vInh_YRJkcaq",
    "outputId": "63b72984-a35c-4a4f-8889-93a59b591f9b"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExFOG5IDh_tk"
   },
   "source": [
    "## Custom Embeddings\n",
    "You can use any embedding that you have previously created. This can be to speed up creating topic models but also if your language model does not fit in any of the options above. \n",
    "\n",
    "Here, we will be using **TF-IDF** as the main embedder in BERTopic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fX9QFYFDh9pU"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "embeddings = TfidfVectorizer(min_df=5, stop_words=\"english\").fit_transform(docs)\n",
    "topic_model = BERTopic().fit(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
